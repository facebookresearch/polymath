from __future__ import annotations

import asyncio
import functools
from logging import Logger
from typing import Any, Callable, Dict, List, Optional, Tuple

from transformers import pipeline, Pipeline

# Project‑local base types ---------------------------------------------------
from inference.chat_completion import ChatCompletion as _BaseChatCompletion, Message, Role
from inference.finish_reason import FinishReason


# ---------------------------------------------------------------------------
# Process‑wide cache (one model at a time) -----------------------------------
# ---------------------------------------------------------------------------
_PIPE: Pipeline | None = None
_PIPE_MODEL_ID: str | None = None
_LOCK = asyncio.Lock()

async def _get_pipeline(model_id: str, *, local_files_only: bool, **pipe_kwargs: Any) -> Pipeline:  # noqa: D401
    """Load a HF text‑generation pipeline once. Async‑safe via a lock."""
    global _PIPE, _PIPE_MODEL_ID

    if _PIPE is not None and _PIPE_MODEL_ID == model_id:
        return _PIPE

    async with _LOCK:  # first waiter loads the model
        if _PIPE is not None and _PIPE_MODEL_ID == model_id:
            return _PIPE

        loop = asyncio.get_running_loop()
        _PIPE = await loop.run_in_executor(
            None,
            functools.partial(
                pipeline,
                task="text-generation",
                model=model_id,
                local_files_only=local_files_only,
                device_map="auto",
                **pipe_kwargs,
            ),
        )
        _PIPE_MODEL_ID = model_id
        return _PIPE

# ---------------------------------------------------------------------------
# Concrete implementation ----------------------------------------------------
# ---------------------------------------------------------------------------
class MistralCompletion(_BaseChatCompletion):
    """Hugging Face *Mistral* implementation compatible with InferenceClient."""

    # Matcher for factory: (logger_factory, model_name, max_gen_tokens,
    # max_tokens, temperature [, top_p])
    def __init__(
        self,
        logger_factory: Callable[[str], Logger] | None,
        model_name: str,
        max_gen_tokens: int,
        max_tokens: int,              # *unused* (pipeline auto‑handles)
        temperature: float,
        top_p: float = 1.0,
        *,
        local_files_only: bool = True,
        **generation_kwargs: Any,
    ) -> None:  # noqa: D401
        self._logger = (logger_factory or (lambda n: Logger(n)))(__name__)
        self._model_id = model_name
        self._max_new_tokens = max_gen_tokens
        self._temperature = temperature
        self._top_p = top_p
        self._local_files_only = local_files_only
        self._generation_kwargs = generation_kwargs

    # ~~~~~~~~~~~~~~~~~~~~~ context‑manager hooks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    async def __aenter__(self) -> "MistralCompletion":
        await _get_pipeline(self._model_id, local_files_only=self._local_files_only)
        return self

    async def __aexit__(self, exc_type, exc, tb):  # noqa: D401
        # Keep the model cached; propagate exceptions.
        return False

    # ~~~~~~~~~~~~~~~~~~~~~~~~~ main entry point ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    async def create(self, conversation: List[Message]) -> Tuple[FinishReason, Optional[str]]:  # noqa: D401
        pipe = await _get_pipeline(self._model_id, local_files_only=self._local_files_only)

        prompt: List[Dict[str, str]] = self._format_conversation(conversation)

        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(
                None,
                functools.partial(
                    pipe,
                    prompt,
                    max_new_tokens=self._max_new_tokens,
                    do_sample=self._temperature > 0,
                    temperature=self._temperature,
                    top_p=self._top_p,
                    num_return_sequences=1,
                    return_full_text=False,
                    **self._generation_kwargs,
                ),
            )
        except RuntimeError as exc:
            # CUDA OOM or similar – treat as retryable.
            self._logger.warning("Pipeline generation failed: %s", exc)
            return FinishReason.RETRYABLE_ERROR, None

        # HF pipeline always returns list[dict].
        generated = result[0]["generated_text"]

        # (1) Plain string → straightforward.
        if isinstance(generated, str):
            text = generated.strip()
            if not text:
                return FinishReason.MAX_OUTPUT_TOKENS, None
            return FinishReason.STOPPED, text

        # (2) List[dict] chat‑template. Find last assistant.
        if isinstance(generated, list):
            for msg in reversed(generated):
                if msg.get("role") == "assistant":
                    return FinishReason.STOPPED, msg.get("content", "").strip()
            # No assistant found → treat as limit.
            return FinishReason.MAX_OUTPUT_TOKENS, None

        # Unexpected shape – signal to caller to retry.
        return FinishReason.RETRYABLE_ERROR, None

    # ---------------------------------------------------------------------
    @staticmethod
    def _format_conversation(conversation: List[Message]) -> List[Dict[str, str]]:  # noqa: D401
        formatted: List[Dict[str, str]] = []
        for msg in conversation:
            role_str = (
                msg.role.value if hasattr(msg.role, "value") else
                msg.role.name.lower() if hasattr(msg.role, "name") else
                str(msg.role).lower()
            )
            formatted.append({"role": role_str, "content": msg.text})
        return formatted
